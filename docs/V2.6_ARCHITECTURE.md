# CyborgMind V2.6 Architecture

**Production-Grade Game AI & RL Brain System**

---

## ğŸ—ï¸ System Overview

CyborgMind V2.6 is a modular, production-hardened reinforcement learning brain designed for:
- Game AI and NPC control
- Minecraft agents (MineRL)
- Gymnasium environments
- Custom RL tasks

### Core Philosophy
1. **Separation of Concerns**: Environment adapters â†” Brain â†” Controller
2. **Universal Interface**: All environments â†’ (pixels, scalars, goal)
3. **Scalability**: Multi-agent support with dynamic memory allocation
4. **Production-Ready**: Docker, monitoring, health checks, validation

---

## ğŸ“ Component Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     CyborgMind V2.6                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Gym Adapter â”‚      â”‚ MineRL Adapt â”‚      â”‚ Syntheticâ”‚ â”‚
â”‚  â”‚              â”‚      â”‚              â”‚      â”‚ Dataset  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â”‚
â”‚         â”‚                     â”‚                   â”‚        â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                               â”‚                            â”‚
â”‚                        [3,128,128] pixels                  â”‚
â”‚                        [scalar_dim] features               â”‚
â”‚                        [goal_dim] objectives               â”‚
â”‚                               â”‚                            â”‚
â”‚                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚                â”‚  BrainCyborgMind (Policy)  â”‚              â”‚
â”‚                â”‚  - Vision Encoder (CLIP)   â”‚              â”‚
â”‚                â”‚  - Fractal RNN (FRNN)      â”‚              â”‚
â”‚                â”‚  - LSTM Workspace          â”‚              â”‚
â”‚                â”‚  - Multi-Head Outputs      â”‚              â”‚
â”‚                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                               â”‚                            â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚                    â”‚  DynamicGPUPMM      â”‚                 â”‚
â”‚                    â”‚  (Memory System)    â”‚                 â”‚
â”‚                    â”‚  - Persistent Slots â”‚                 â”‚
â”‚                    â”‚  - Attention Read   â”‚                 â”‚
â”‚                    â”‚  - Eviction Policy  â”‚                 â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚                                                             â”‚
â”‚                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚                â”‚ CyborgMindController    â”‚                 â”‚
â”‚                â”‚ - Multi-agent Mgmt      â”‚                 â”‚
â”‚                â”‚ - Memory Allocation     â”‚                 â”‚
â”‚                â”‚ - Metrics Collection    â”‚                 â”‚
â”‚                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ§© Layer-by-Layer Breakdown

### Layer 1: Environment Adapters

**Purpose:** Convert any environment into brain-compatible format

**Gymnasium Adapter (`gym_adapter.py`)**
- **Input:** Gym/Gymnasium environment
- **Output:** `BrainInputs(pixels, scalars, goal)`
- **Features:**
  - Auto-detection of observation/action spaces
  - Continuous â†’ discrete action mapping
  - NaN/inf validation
  - Both gym and gymnasium support

**MineRL Adapter (`minerl_adapter.py`)**
- **Input:** MineRL environment
- **Output:** `BrainInputs(pixels, scalars, goal)`
- **Features:**
  - POV extraction from dict observations
  - Inventory/compass â†’ scalar features
  - 19 discrete action combinations
  - Camera look smoothing

**Synthetic Dataset (`synthetic_dataset.py`)**
- **Purpose:** Generate training data
- **Features:**
  - Deterministic seeded generation
  - Configurable difficulty
  - Stable trajectories

---

### Layer 2: Brain (BrainCyborgMind)

**Located:** `cyborg_mind_v2/capsule_brain/policy/brain_cyborg_mind.py`

#### Input Processing
```python
# Vision pathway
pixels [B, 3, 128, 128] 
  â†’ CLIP Vision Encoder 
  â†’ vision_features [B, vision_dim]

# Scalar pathway
scalars [B, scalar_dim]
  â†’ MLP
  â†’ scalar_features [B, hidden_dim]

# Goal pathway
goal [B, goal_dim]
  â†’ Embedding
  â†’ goal_features [B, hidden_dim]
```

#### Core Processing
```python
# Fusion
fused = concat(vision_features, scalar_features, goal_features)

# Fractal RNN (hierarchical time-scales)
frnn_out, frnn_hidden = FRNN(fused, prev_frnn_hidden)

# Memory interaction
memory_query = linear(frnn_out)
memory_context, attention = PMM.read(memory_query)

# Workspace (LSTM for working memory)
workspace_in = concat(frnn_out, memory_context)
workspace_out, lstm_hidden = LSTM(workspace_in, prev_lstm_hidden)
```

#### Output Heads
```python
# Policy head
action_logits = action_head(workspace_out)  # [B, action_dim]

# Value head
value = value_head(workspace_out)  # [B, 1]

# Thought head (latent reasoning)
thought = thought_head(workspace_out)  # [B, thought_dim]

# Emotion head (internal state)
emotion = emotion_head(workspace_out)  # [B, emotion_dim]
```

#### Memory Write
```python
# Consolidate experience
write_vector = concat(workspace_out, thought, emotion)
PMM.write(write_vector)
```

---

### Layer 3: Memory (DynamicGPUPMM)

**Located:** `cyborg_mind_v2/capsule_brain/policy/brain_cyborg_mind.py`

#### Architecture
- **Slots:** Fixed number of memory slots (e.g., 256)
- **Dimensions:** Each slot holds a vector [mem_dim]
- **Addressing:** Cosine similarity attention

#### Operations

**Read (Attention-based)**
```python
# Compute similarity
similarity = cosine_sim(query, all_memory_slots)  # [B, num_slots]

# Apply temperature
attention = softmax(similarity / temperature)  # [B, num_slots]

# Weighted read
context = attention @ memory  # [B, mem_dim]
```

**Write (Priority-based)**
```python
# Compute pressure (how important is this memory?)
pressure = compute_pressure(write_vector)

# Find eviction candidate (lowest priority)
evict_idx = argmin(slot_priorities)

# Write to slot
memory[evict_idx] = write_vector
priorities[evict_idx] = pressure
```

**V2.6 Enhancements:**
- âœ… Write clamping prevents overflow
- âœ… Adaptive eviction thresholds
- âœ… Min-slot protection (never evict critical memories)
- âœ… Stable normalization

---

### Layer 4: Controller (CyborgMindController)

**Located:** `cyborg_mind_v2/integration/cyborg_mind_controller.py`

#### Multi-Agent Management

```python
# Each agent gets:
{
    "agent_id": unique identifier,
    "brain_slot": index in batch,
    "memory_slot": dedicated PMM slots,
    "hidden_states": LSTM/FRNN states,
    "last_action": previous action,
    "last_value": value estimate,
    "last_pressure": memory pressure,
    "steps_inactive": timeout counter
}
```

#### Lifecycle
1. **Agent Join:** Allocate slot, initialize hidden states
2. **Agent Step:** Forward pass, update states, collect metrics
3. **Agent Idle:** Increment inactive counter
4. **Agent Leave:** Cleanup slot, save final state
5. **Auto Cleanup:** Remove after N inactive steps

#### V2.6 Enhancements:
- âœ… Exception-safe action fallback
- âœ… Pressure-driven memory expansion
- âœ… Per-agent metrics tracking

---

## ğŸ”„ Data Flow Example

### Episode Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Environment Reset                                     â”‚
â”‚    env.reset() â†’ raw_obs                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. Adapter Processing                                    â”‚
â”‚    adapter.reset() â†’ BrainInputs(pixels, scalars, goal)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. Controller Step                                       â”‚
â”‚    controller.step(pixels, scalars, goal) â†’ action_idx   â”‚
â”‚    - Lookup agent slot                                   â”‚
â”‚    - Load hidden states                                  â”‚
â”‚    - Forward pass through brain                          â”‚
â”‚    - Memory read/write                                   â”‚
â”‚    - Sample action from logits                           â”‚
â”‚    - Save new hidden states                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. Adapter Execution                                     â”‚
â”‚    adapter.step(action_idx) â†’ (obs, reward, done, info)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. Training Update (PPO)                                 â”‚
â”‚    - Collect rollout (N steps)                           â”‚
â”‚    - Compute advantages (GAE)                            â”‚
â”‚    - Update policy (clip objective)                      â”‚
â”‚    - Update value function (MSE)                         â”‚
â”‚    - Checkpoint model                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¯ Training Pipeline

### Teacher-Student Distillation

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ RealTeacher (CLIP-based)           â”‚
â”‚ - Pre-trained vision encoder       â”‚
â”‚ - Frozen CLIP weights              â”‚
â”‚ - Provides soft targets            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚ KL Divergence Loss
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Student (BrainCyborgMind)          â”‚
â”‚ - Learns to mimic teacher          â”‚
â”‚ - Compresses knowledge             â”‚
â”‚ - Adds FRNN/LSTM/Memory            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### PPO Training Loop

```python
for epoch in range(num_epochs):
    # Collect rollout
    for step in range(rollout_steps):
        action, value, log_prob = brain(obs)
        next_obs, reward, done = env.step(action)
        buffer.store(obs, action, reward, value, log_prob)
    
    # Compute returns
    advantages = compute_gae(buffer.rewards, buffer.values)
    returns = advantages + buffer.values
    
    # Update policy
    for _ in range(ppo_epochs):
        # Sample mini-batch
        batch = buffer.sample()
        
        # Forward pass
        new_log_probs, values, entropy = brain.evaluate(batch)
        
        # Policy loss (clipped)
        ratio = exp(new_log_probs - batch.old_log_probs)
        clipped_ratio = clip(ratio, 1-eps, 1+eps)
        policy_loss = -min(ratio * advantages, clipped_ratio * advantages)
        
        # Value loss
        value_loss = mse(values, returns)
        
        # Total loss
        loss = policy_loss + value_coef * value_loss - entropy_coef * entropy
        
        # Backprop
        loss.backward()
        clip_grad_norm_(brain.parameters(), max_grad_norm)
        optimizer.step()
    
    # Checkpoint
    if epoch % save_interval == 0:
        save_checkpoint(brain, optimizer, epoch)
```

---

## ğŸ“Š Monitoring & Observability

### Prometheus Metrics

**Brain Metrics**
- `cyborg_actions_total`: Actions taken per agent
- `cyborg_value_estimate`: Value head predictions
- `cyborg_thought_norm`: Thought vector magnitude
- `cyborg_emotion_divergence`: Emotion drift

**Memory Metrics**
- `cyborg_memory_pressure_avg`: Average memory pressure
- `cyborg_memory_slots_used`: Active memory slots
- `cyborg_memory_evictions_total`: Eviction count

**Environment Metrics**
- `cyborg_episode_reward`: Cumulative reward
- `cyborg_episode_length`: Steps per episode
- `cyborg_minerl_latency_seconds`: MineRL step time

**Error Metrics**
- `cyborg_nan_events_total`: NaN detections
- `cyborg_errors_total`: Exception count

### Grafana Dashboards

**Dashboard 1: Performance**
- Episode rewards over time
- Actions per second
- Value prediction accuracy

**Dashboard 2: Memory**
- Memory pressure heatmap
- Slot utilization
- Eviction rate

**Dashboard 3: Errors**
- NaN detection timeline
- Error rate by component

---

## ğŸš€ Deployment Patterns

### Pattern 1: Single Agent (Development)
```bash
python train.py --env CartPole-v1 --episodes 1000
```

### Pattern 2: Multi-Agent (Production)
```bash
docker-compose up -d
curl -X POST http://localhost:8000/agent/create -d '{"env": "MineRLTreechop-v0"}'
```

### Pattern 3: Distributed Training
```python
# Not yet implemented in V2.6
# Coming in V2.7
```

---

## ğŸ”§ Configuration

### Environment Variables
```bash
CUDA_VISIBLE_DEVICES=0       # GPU selection
LOG_LEVEL=INFO               # Logging verbosity
TORCH_HOME=/app/models       # Model cache
WANDB_MODE=online            # Experiment tracking
```

### Config File
```yaml
# config.yaml
brain:
  vision_dim: 512
  hidden_dim: 256
  thought_dim: 128
  emotion_dim: 64
  memory_slots: 256

training:
  lr: 3e-4
  gamma: 0.99
  gae_lambda: 0.95
  ppo_epochs: 10
  clip_epsilon: 0.2
```

---

## ğŸ“š Further Reading

- `docs/ADAPTER_SYSTEM.md` - Creating custom adapters
- `docs/TRAINING_README.md` - Training best practices
- `docs/V2.6_DEPLOYMENT.md` - Production deployment
- `docs/V2.6_MONITORING.md` - Metrics and alerts

---

**For support, open an issue on GitHub.**
